{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm,tgrange\n",
    "import os\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, LabeledSentence\n",
    "TaggededDocument = gensim.models.doc2vec.TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 准备预训练语料：G-06-F-17类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus_filename = \"/home/hxjiang/Pythonworkspace/patent/sample3_G-06-F-17/textual/patent_text_2007_description.xlsx\"\n",
    "corpus = pd.read_excel(corpus_filename, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"number of corpus：\"+str(len(corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 分词等清洗工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus['description'].dropna(inplace=True)\n",
    "# corpus['description'] = [entry.lower() for entry in corpus['description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus['description'] = [ word_tokenize(entry) for entry in corpus['description'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "\n",
    "for index, entry in enumerate(tqdm(corpus['description'], ncols=60)):\n",
    "    Final_words = []\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]]) # 对这三类做词形还原\n",
    "            Final_words.append(word_Final)\n",
    "    corpus.loc[index,'description_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 将语料整理成规定的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_train = []\n",
    "\n",
    "for i, text in enumerate(tqdm(corpus['description_final'], ncols=60)):\n",
    "    document = TaggededDocument(text, tags=[i])\n",
    "    x_train.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train(x_train):\n",
    "    doc_model = Doc2Vec(x_train, min_count=1, window=5, vector_size=300, sample=1e-3, negative=5, workers=2)\n",
    "    doc_model.train(x_train, total_examples=doc_model.corpus_count, epochs=20) # corpus_count是文件个数，epochs训练次数\n",
    "    return doc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc_model = train(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc_model.save(\"G-06-F-17_300d.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 测试模型效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 计算余弦相似度\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = 0.0\n",
    "    normA = 0.0\n",
    "    normB = 0.0\n",
    "    for a, b in zip(vector1, vector2):\n",
    "        dot_product += a * b\n",
    "        normA += a ** 2\n",
    "        normB += b ** 2\n",
    "    if normA == 0.0 or normB == 0.0:\n",
    "        return 0\n",
    "    else:\n",
    "        return round(dot_product / ((normA**0.5)*(normB**0.5)) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 5.得到两个句子的向量后使用余弦相似度\n",
    "text1 = \"One embodiment of the present invention provides a system for facilitating social networking based on fashion-related information. During operation, the system receives fashion-related information from a user. Next, the system extracts the user\\\\\\'s fashion preferences from the received information and compares the user\\\\\\'s fashion preference with other users\\\\\\' fashion preferences. Finally, the system groups users based on similarity of their fashion preferences.\"\n",
    "text2 = \"novel multimodal learning setting of company earnings conference call\"\n",
    "text1 = text1.split()\n",
    "text2 = text2.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc_model = Doc2Vec.load(\"G-06-F-17_300d.model\")\n",
    "text1_inferred_vector = doc_model.infer_vector(text1)\n",
    "doc_model = Doc2Vec.load(\"G-06-F-17_300d.model\")\n",
    "text2_inferred_vector = doc_model.infer_vector(text2)\n",
    "cos = cosine_similarity(text1_inferred_vector, text2_inferred_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 得到文档向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_filename = \"/home/hxjiang/Pythonworkspace/patent/sample3_G-06-F-17/textual/patent_text_2010.xlsx\"\n",
    "load_data = pd.read_excel(load_filename, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"number of corpus：\"+str(len(load_data['abstract'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "load_data['abstract'].dropna(inplace=True)\n",
    "# load_data['abstract'] = [entry.lower() for entry in load_data['abstract']]\n",
    "load_data['abstract'] = [word_tokenize(entry) for entry in load_data['abstract']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embedding_file = \"/home/hxjiang/Pythonworkspace/patent/sample3_G-06-F-17/textual/abstract_doc2vec.csv\"\n",
    "\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "for index, entry in enumerate(tqdm(load_data['abstract'], ncols=60)):\n",
    "    Final_words = []\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    doc_model = Doc2Vec.load(\"G-06-F-17_300d.model\")\n",
    "    text_inferred_vector = doc_model.infer_vector(Final_words)\n",
    "    text_inferred_vector = text_inferred_vector * 100\n",
    "    vector_df = pd.DataFrame([text_inferred_vector])\n",
    "    if os.path.exists(embedding_file):\n",
    "        vector_df.to_csv(embedding_file, header=0, mode='a', index=False, sep=',')\n",
    "    else:\n",
    "        vector_df.to_csv(embedding_file, mode='a', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data['claims'].dropna(inplace=True)\n",
    "# load_data['claims'] = [entry.lower() for entry in load_data['claims']]\n",
    "load_data['claims'] = [word_tokenize(entry) for entry in load_data['claims']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = \"/home/hxjiang/Pythonworkspace/patent/sample3_G-06-F-17/textual/claims_doc2vec.csv\"\n",
    "\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "for index, entry in enumerate(tqdm(load_data['claims'], ncols=60)):\n",
    "    Final_words = []\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    doc_model = Doc2Vec.load(\"G-06-F-17_300d.model\")\n",
    "    text_inferred_vector = doc_model.infer_vector(Final_words)\n",
    "    text_inferred_vector = text_inferred_vector * 100\n",
    "    vector_df = pd.DataFrame([text_inferred_vector])\n",
    "    if os.path.exists(embedding_file):\n",
    "        vector_df.to_csv(embedding_file, header=0, mode='a', index=False, sep=',')\n",
    "    else:\n",
    "        vector_df.to_csv(embedding_file, mode='a', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "176px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
