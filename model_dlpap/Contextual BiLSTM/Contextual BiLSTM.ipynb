{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T07:15:22.575209Z",
     "start_time": "2020-04-18T07:15:21.329226Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T07:15:23.704516Z",
     "start_time": "2020-04-18T07:15:23.646488Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T07:15:25.866505Z",
     "start_time": "2020-04-18T07:15:24.571335Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import layers, models, optimizers\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T07:15:32.569690Z",
     "start_time": "2020-04-18T07:15:28.400989Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"/home/hxjiang/Pythonworkspace/patent/sample3_G-06-F-17/textual/after_process.xlsx\"\n",
    "data = pd.read_excel(filename, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T07:15:33.311663Z",
     "start_time": "2020-04-18T07:15:33.293822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>application_id</th>\n",
       "      <th>claims</th>\n",
       "      <th>claims_add1</th>\n",
       "      <th>claims_add2</th>\n",
       "      <th>location</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract_sen_count</th>\n",
       "      <th>claims_sen_count</th>\n",
       "      <th>abstract_final</th>\n",
       "      <th>abstract_count</th>\n",
       "      <th>claims_final</th>\n",
       "      <th>claims_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['A', 'method', 'of', 'providing', 'a', 'secur...</td>\n",
       "      <td>11041610</td>\n",
       "      <td>['1', '.', 'A', 'method', 'of', 'providing', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010/ipa100107/US20100001069A1.xml</td>\n",
       "      <td>METHOD OF PRINTING SECURITY DOCUMENTS</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>['A', 'method', 'provide', 'security', 'docume...</td>\n",
       "      <td>46</td>\n",
       "      <td>['A', 'method', 'provide', 'security', 'docume...</td>\n",
       "      <td>622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  application_id  \\\n",
       "0  ['A', 'method', 'of', 'providing', 'a', 'secur...        11041610   \n",
       "\n",
       "                                              claims claims_add1 claims_add2  \\\n",
       "0  ['1', '.', 'A', 'method', 'of', 'providing', '...         NaN         NaN   \n",
       "\n",
       "                             location                                  title  \\\n",
       "0  2010/ipa100107/US20100001069A1.xml  METHOD OF PRINTING SECURITY DOCUMENTS   \n",
       "\n",
       "   abstract_sen_count  claims_sen_count  \\\n",
       "0                   4                19   \n",
       "\n",
       "                                      abstract_final  abstract_count  \\\n",
       "0  ['A', 'method', 'provide', 'security', 'docume...              46   \n",
       "\n",
       "                                        claims_final  claims_count  \n",
       "0  ['A', 'method', 'provide', 'security', 'docume...           622  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = pd.read_excel(r\"/home/hxjiang/Pythonworkspace/patent/sample3_G-06-F-17/textual/2010_result.xlsx\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>application_id</th>\n",
       "      <th>inventor_num</th>\n",
       "      <th>inventor_patent_num</th>\n",
       "      <th>assignee_num</th>\n",
       "      <th>assignee_patent_num</th>\n",
       "      <th>claims_num</th>\n",
       "      <th>famliy</th>\n",
       "      <th>cpc_class</th>\n",
       "      <th>pa_country</th>\n",
       "      <th>pa_state</th>\n",
       "      <th>pa_city</th>\n",
       "      <th>result</th>\n",
       "      <th>filed_time</th>\n",
       "      <th>published_time</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>11041610</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2684.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>G-06-F-17</td>\n",
       "      <td>AU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BALMAIN</td>\n",
       "      <td>1</td>\n",
       "      <td>2005-01-25</td>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>2010/ipa100107/US20100001069A1.xml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    application_id  inventor_num  inventor_patent_num  assignee_num  \\\n",
       "37        11041610           2.0               2684.0           1.0   \n",
       "\n",
       "    assignee_patent_num  claims_num  famliy  cpc_class pa_country pa_state  \\\n",
       "37                 48.0          19       0  G-06-F-17         AU      NaN   \n",
       "\n",
       "    pa_city  result filed_time published_time  \\\n",
       "37  BALMAIN       1 2005-01-25     2010-01-07   \n",
       "\n",
       "                              location  \n",
       "37  2010/ipa100107/US20100001069A1.xml  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 划分训练集和测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T07:15:36.061993Z",
     "start_time": "2020-04-18T07:15:36.054527Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_abs_train = data['abstract_final'].iloc[:6215]\n",
    "x_abs_valid = data['abstract_final'].iloc[6215:6992]\n",
    "x_abs_test = data['abstract_final'].iloc[6992:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T07:15:38.727119Z",
     "start_time": "2020-04-18T07:15:38.719464Z"
    }
   },
   "outputs": [],
   "source": [
    "x_claims_train = data['claims_final'].iloc[:6215]\n",
    "x_claims_valid = data['claims_final'].iloc[6215:6992]\n",
    "x_claims_test = data['claims_final'].iloc[6992:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不平衡数据权重调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T07:15:46.462860Z",
     "start_time": "2020-04-18T07:15:46.440244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.49346405, 0.75164474])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target = np_utils.to_categorical(result_data[['result']], 2)\n",
    "y_ints = [y.argmax() for y in train_target]\n",
    "cw = class_weight.compute_class_weight('balanced', np.unique(y_ints), y_ints)\n",
    "cw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T07:15:47.096334Z",
     "start_time": "2020-04-18T07:15:47.090000Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7769, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T07:15:47.635270Z",
     "start_time": "2020-04-18T07:15:47.630731Z"
    }
   },
   "outputs": [],
   "source": [
    "y_binary_train = result_data['result'][:6215]\n",
    "y_binary_valid = result_data['result'][6215:6992]\n",
    "y_binary_test = result_data['result'][6992:]\n",
    "\n",
    "y_category_train = train_target[:6215]\n",
    "y_category_valid = train_target[6215:6992]\n",
    "y_category_test = train_target[6992:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T07:15:48.172723Z",
     "start_time": "2020-04-18T07:15:48.161001Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9765325670498084"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_category_train[:, 1])/sum(y_category_train[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以词向量为特征\n",
    "词向量是一种利用稠密向量表示词或者文档的形式，词在向量空间中的位置从文本中学习得到并且以该词附近出现的词为学习依据。\n",
    "词向量可以由输入语料自身学习得到或者可以利用预训练好的词向量生成，例如Glove，FastText和word2Vec。\n",
    "预训练词向量有四个必要的步骤：\n",
    "1. 加载预训练的词向量\n",
    "2. 创建标记器对象\n",
    "3. 将文本文档转换为词条序列并对其进行填补。\n",
    "4. 创建词条与其对应的词向量之间的映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T07:17:36.543610Z",
     "start_time": "2020-04-18T07:16:59.551418Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400001it [00:36, 10816.36it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "for line in tqdm(open('/home/hxjiang/Pythonworkspace/patent/sample3_G-06-F-17/textual/glove.6B.300d.word2vec.txt', 'r', encoding='UTF-8')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T07:16:27.850683Z",
     "start_time": "2020-04-18T07:16:26.598475Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17403"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_token = text.Tokenizer()\n",
    "abstract_token.fit_on_texts(data['abstract'])\n",
    "abstract_word_index = abstract_token.word_index\n",
    "len(abstract_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T07:16:36.985773Z",
     "start_time": "2020-04-18T07:16:36.237105Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_abs_train_seq = sequence.pad_sequences(abstract_token.texts_to_sequences(x_abs_train), maxlen=140)\n",
    "x_abs_valid_seq = sequence.pad_sequences(abstract_token.texts_to_sequences(x_abs_valid), maxlen=140)\n",
    "x_abs_test_seq = sequence.pad_sequences(abstract_token.texts_to_sequences(x_abs_test), maxlen=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T06:36:57.247821Z",
     "start_time": "2020-04-18T06:34:02.038Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix_abstract.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18594"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claims_token = text.Tokenizer()\n",
    "claims_token.fit_on_texts(data['claims_final'])\n",
    "claims_word_index = claims_token.word_index\n",
    "len(claims_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_claims_train_seq = sequence.pad_sequences(claims_token.texts_to_sequences(x_claims_train), maxlen=1400)\n",
    "x_claims_valid_seq = sequence.pad_sequences(claims_token.texts_to_sequences(x_claims_valid), maxlen=1400)\n",
    "x_claims_test_seq = sequence.pad_sequences(claims_token.texts_to_sequences(x_claims_test), maxlen=1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_claims = np.zeros((len(claims_word_index) + 1, 300)) # 50是词向量的维度,+1 is because the matrix indices start with 0\n",
    "for word, i in tqdm(claims_word_index.items(), ncols=70):\n",
    "    word = word.strip('\\'')\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_claims[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_claims.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM捕捉语序信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def fusion_network():\n",
    "    # for abstract\n",
    "    input_layer_abstract = layers.Input((SEQ_LEN_abstract, ))\n",
    "    embedding_layer_abstract = layers.Embedding(input_dim=len(VOCAB_SIZE_abstract) + 1,\n",
    "                                       output_dim=EMBEDDING_DIM,\n",
    "                                       weights=[embedding_matrix_abstract],\n",
    "                                       mask_zero=True,\n",
    "                                       trainable=False)(input_layer_abstract)    \n",
    "    # embedding_layer_abstract = layers.SpatialDropout1D(dropout)(embedding_layer_abstract)\n",
    "    lstm_layer_abstract = layers.Bidirectional(LSTM(abs_lstm_num, return_sequences=False))(embedding_layer_abstract)\n",
    "    dense_abstract_1 = layers.Dense(abs_dense_num, activation=abs_dense_active)(lstm_layer_abstract)\n",
    "    dropout_abstract_1 = layers.Dropout(dropout)(dense_abstract_1)\n",
    "    output_layer_abstract = layers.Dense(2, activation=abs_output_active)(dropout_abstract_1)\n",
    "    abstract_model = models.Model(inputs=input_layer_abstract, outputs=output_layer_abstract)\n",
    "    abstract_model.compile(optimizer=abs_optimizer, loss=abs_loss, metrics=['accuracy'])\n",
    "\n",
    "    # for claims\n",
    "    input_layer_claims = layers.Input((SEQ_LEN_claims, ))\n",
    "    embedding_layer_claims = layers.Embedding(input_dim=len(VOCAB_SIZE_claims) + 1,\n",
    "                                       output_dim=EMBEDDING_DIM,\n",
    "                                       weights=[embedding_matrix_claims],\n",
    "                                       mask_zero=True,\n",
    "                                       trainable=False)(input_layer_claims)\n",
    "    # embedding_layer_claims = layers.SpatialDropout1D(0.3)(embedding_layer_claims)\n",
    "    lstm_layer_claims = layers.Bidirectional(LSTM(claims_lstm_num, dropout=0.3, return_sequences=False))(embedding_layer_claims)\n",
    "    # model1.add(Bidirectional(LSTM(64,dropout=0.4, recurrent_dropout=0.4),merge_mode='concat'))\n",
    "    dense_claims_1 = layers.Dense(64, activation=\"relu\")(lstm_layer_claims)\n",
    "    dropout_claims_1 = layers.Dropout(dropout)(dense_claims_1)\n",
    "    output_layer_claims = layers.Dense(2, activation=\"softmax\")(dropout_claims_1)\n",
    "    claims_model = models.Model(inputs=input_layer_claims, outputs=output_layer_claims)\n",
    "    claims_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # for fusion\n",
    "    fusion_dense_layer = layers.Dense(256, activation='relu')(concatenate([lstm_layer_abstract, lstm_layer_claims]))\n",
    "    fusion_dropout_1 = layers.Dropout(dropout)(fusion_dense_layer)\n",
    "    # flatten_1 = Flatten(name='flatten_1')(dropout_1)\n",
    "    fusion_dense = layers.Dense(64, activation='relu')(fusion_dropout_1)\n",
    "    fusion_dropout_2 = layers.Dropout(dropout)(fusion_dense)\n",
    "    # flatten_2 = Flatten(name='flatten_1')(dropout_2)\n",
    "    output_layer_fusion = layers.Dense(2, activation=\"softmax\")(fusion_dropout_2)    \n",
    "    fusion_model = models.Model(inputs=[input_layer_abstract, input_layer_claims], output=output_layer_fusion)\n",
    "    fusion_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return abstract_model, claims_model, fusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = embedding_matrix_abstract.shape[1]\n",
    "SEQ_LEN_abstract = x_abs_train_seq.shape[1]\n",
    "SEQ_LEN_claims = x_claims_train_seq.shape[1]\n",
    "VOCAB_SIZE_abstract = abstract_word_index\n",
    "VOCAB_SIZE_claims = claims_word_index\n",
    "\n",
    "abs_lstm_num = 300\n",
    "abs_dense_num = 32 # 64/32\n",
    "abs_dense_active = \"relu\"\n",
    "abs_output_active = \"softmax\"\n",
    "abs_optimizer = optimizers.Adam()\n",
    "abs_loss = 'binary_crossentropy'\n",
    "Epoch = 40\n",
    "BATCH_SIZE = 32\n",
    "dropout = 0.5 # 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abstract_model, claims_model, fusion_model = fusion_network()\n",
    "\n",
    "# abstract\n",
    "filepath = './-'+str(abs_dense_num)+'-'+str(dropout)+'-AbsModel-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "abs_history = abstract_model.fit(x_abs_train_seq, y_category_train, validation_data=(x_abs_valid_seq, y_category_valid), \n",
    "                verbose=1, epochs=Epoch, batch_size=BATCH_SIZE, shuffle=False, callbacks=[checkpoint], class_weight=cw)\n",
    "# abstract_model.save(\"./abstract_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = abstract_model.predict(x_abs_test_seq, batch_size=BATCH_SIZE, verbose=0)\n",
    "# y_pred_1 = np.rint(y_pred) #四舍五入取整\n",
    "for i in range(len(y_pred)):\n",
    "    max_value=max(y_pred[i])\n",
    "    for j in range(len(y_pred[i])):\n",
    "        if max_value==y_pred[i][j]:\n",
    "            y_pred[i][j]=1\n",
    "        else:\n",
    "            y_pred[i][j]=0\n",
    "report = classification_report(y_category_test, y_pred, digits=4)\n",
    "p = re.compile('  |\\n', re.S)\n",
    "report = p.sub(' ', report)\n",
    "metrics_content = re.findall(\"([\\d]{1}\\.[\\d]{4})    777\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"acc:  {:.4f}\".format(metrics_content[0]))\n",
    "print(\"macro:  {:.4f}\".format(metrics_content[1]))\n",
    "print(\"weighted:  {:.4f}\".format(metrics_content[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_category_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_history(abs_history_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, acc = claims_model.evaluate(x_claims_test_seq, y_category_test, batch_size=BATCH_SIZE, verbose=1)\n",
    "# print(\"accuracy: {:.4f} loss: {:.4f}\".format(acc,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_metrics_list = []\n",
    "for model in tqdm(claims_model_list, ncols=70):\n",
    "    y_pred = model.predict(x_claims_test_seq, batch_size=BATCH_SIZE, verbose=0)\n",
    "    y_pred_1 = np.rint(y_pred) #四舍五入取整\n",
    "    for i in range(len(y_pred)):\n",
    "        max_value=max(y_pred[i])\n",
    "        for j in range(len(y_pred[i])):\n",
    "            if max_value==y_pred[i][j]:\n",
    "                y_pred[i][j]=1\n",
    "            else:\n",
    "                y_pred[i][j]=0\n",
    "    report = classification_report(y_category_test, y_pred, digits=4)\n",
    "    p = re.compile('  |\\n', re.S)\n",
    "    report = p.sub(' ', report)\n",
    "    metrics_content = re.findall(\"([\\d]{1}\\.[\\d]{4})    777\", report)\n",
    "    claims_metrics_list.append(metrics_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_metrics_arrat = np.array(claims_metrics_list,dtype=float)\n",
    "acc = np.mean(claims_metrics_arrat[:,0])\n",
    "print(\"acc:  {:.4f}\".format(acc))\n",
    "f1 = np.mean(claims_metrics_arrat[:,1])\n",
    "print(\"macro:  {:.4f}\".format(f1))\n",
    "weighted = np.mean(claims_metrics_arrat[:,2])\n",
    "print(\"weighted:  {:.4f}\".format(weighted))\n",
    "\n",
    "print(claims_metrics_arrat[:,0].tolist())\n",
    "print(claims_metrics_arrat[:,1].tolist())\n",
    "print(claims_metrics_arrat[:,2].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_category_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, acc = fusion_model.evaluate([x_abs_test_seq, x_claims_test_seq], test_y, batch_size=BATCH_SIZE, verbose=1)\n",
    "# print(\"accuracy: {:.4f} loss: {:.4f}\".format(acc,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_metrics_list = []\n",
    "for model in tqdm(fusion_model_list, ncols=70):\n",
    "    y_pred = model.predict([x_abs_test_seq,x_claims_test_seq], batch_size=BATCH_SIZE, verbose=0)\n",
    "    y_pred_1 = np.rint(y_pred) #四舍五入取整\n",
    "    for i in range(len(y_pred)):\n",
    "        max_value=max(y_pred[i])\n",
    "        for j in range(len(y_pred[i])):\n",
    "            if max_value==y_pred[i][j]:\n",
    "                y_pred[i][j]=1\n",
    "            else:\n",
    "                y_pred[i][j]=0\n",
    "    report = classification_report(y_category_test, y_pred, digits=4)\n",
    "    p = re.compile('  |\\n', re.S)\n",
    "    report = p.sub(' ', report)\n",
    "    metrics_content = re.findall(\"([\\d]{1}\\.[\\d]{4})    777\", report)\n",
    "    fusion_metrics_list.append(metrics_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_metrics_arrat = np.array(fusion_metrics_list,dtype=float)\n",
    "acc = np.mean(fusion_metrics_arrat[:,0])\n",
    "print(\"acc:  {:.4f}\".format(acc))\n",
    "f1 = np.mean(fusion_metrics_arrat[:,1])\n",
    "print(\"macro:  {:.4f}\".format(f1))\n",
    "weighted = np.mean(fusion_metrics_arrat[:,2])\n",
    "print(\"weighted:  {:.4f}\".format(weighted))\n",
    "\n",
    "print(fusion_metrics_arrat[:,0].tolist())\n",
    "print(fusion_metrics_arrat[:,1].tolist())\n",
    "print(fusion_metrics_arrat[:,2].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_category_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_history(fusion_history_list[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "196px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 277,
   "position": {
    "height": "299px",
    "left": "967px",
    "right": "20px",
    "top": "8px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
